risk <- "rainfall_tomorrow"
#Note available variable ignoring identifiers and risk, with target first
ds %>%
names() %>%
setdiff(c(id,risk)) %>%
c(target, .) %>%
unique() %T>%
print()->
vars
#We will want to ignore some variables that are not relevant to our analysis
ignore <- NULL
#Remove variable to ignore from the variable list
vars%<>%setdiff(ignore) %T>% print()
#Deal with numeric variables
#Fix specific numeric variables
cvars <- c("evaporation", "sunshine")
ds[cvars] %<>% sapply(as.numeric)
#Remove observations without a value for rain_tomorrow
ds%<>% drop_na(rain_tomorrow)
#Identify character variables by index
chari <-ds[vars] %>% sapply(is.character) %>% which() %T>% print()
#Identify character variables by name
charc <-ds[vars] %>% names() %>% '['(chari) %T>% print()
#Observe unique levels
ds[charc] %>% sapply (unique)
#Convert all character to factor if determined appropriate
ds[charc]%<>%map(factor)
#Take a quick glimpse into the dataset
glimpse(ds)
#Visualise relationship in the dataset
ds %>%
filter(location%in%c("Canberra", "Darwin", "Sydney")) %>%
filter(temp_3pm %>% is.na() %>% not()) %>%
select(temp_3pm, location) %>%
ggplot(aes(x=temp_3pm, colour=location, fill=location))+
geom_density(alpha=0.55)
#Formula for modelling
ds[vars] %>%
formula() %T>%
print() ->
form
#Ensure that the target is categoric
ds[[target]] %<>% factor()
#Identify the input variables by name
inputs <-setdiff(vars,target) %T>% print()
#Record the number of observations
nobs <- nrow(ds) %T>% comcat()
#Initialise random numbers for repeatedable results
seed <-123
set.seed(seed)
#Partition the full dataset into three: train(70%), validate (15%), test(15%)
nobs %>%
sample(0.70*nobs) %T>%
{length(.) %>% comma() %>% cat("Size of training dataset:",.,"\n")} ->
train
#Create a validation dataset of 15% of the observations
nobs %>%
seq_len() %>%
setdiff(train) %>%
sample(0.15*nobs) %T>%
{length(.) %>% comma() %>% cat("Size of validation dataset:",.,"\n")}->
validate
#Create a testing dataset of 15%(the remainder) of the observations
nobs %>%
seq_len() %>%
setdiff(union(train, validate)) %T>%
{length(.) %>% comma() %>% cat("Size of validation dataset:",.,"\n")}->
test
#Cache the various actual values for target and risk
tr_target <-ds[train,][[target]] %T>% {head(., 15) %>% print()}
tr_risk <-ds[train,][[risk]] %T>% {head(., 15) %>% print()}
va_target <-ds[validate,][[target]] %T>% {head(., 15) %>% print()}
va_risk <- ds[validate,][[risk]] %T>% {head(., 15) %>% print()}
te_target <-ds[test,][[target]] %T>% {head(., 15) %>% print()}
te_risk <- ds[test,][[risk]] %T>% {head(., 15) %>% print()}
#splitting function: "anova" "poisson" "class" "exp"
mthd <- "class"
#Splitting function parameters
prms <- list(split= "information")
#Control the training
ctrl <-rpart.control(maxdepth=5)
#Build the model
m_rp <-rpart(form, ds[train, vars], method=mthd, parms=prms, control=ctrl)
library(randomForest)
#Control the training
ctrl <-rpart.control(maxdepth=5)
#I am building a decision tree model to predict future events from historic observations of that event.
# The event is whether it will rain tomorrow.
#Load required packages from local library into R
library(tidyverse)
library(rattle)
library(magrittr)
library(lubridate)
library(stringi)
library(stringr)
library(randomForest)
library(FSelector)
library(scales)
library(xtable)
#Original dataset source/location
dsorig <- file.path("https://rattle.togaware.com/weatherAUS.csv")
#Name of the dataset
dsname <-"weatherAUS"
#Identify the essential location of the dataset
dsloc <- "C:/Users/EDJO/Documents/Data Cleaning/Data"
dspath <- file.path(dsloc, dsname%s+% ".csv") %T>% print()
#Ingest the dataset
dspath %>% read_csv() %>% assign(dsname, ., envir=.GlobalEnv)
#Store the dataset with a generic template variable name
dsname %>% get() %T>% print() -> ds
#Normalise variable names
names(ds) %<>% normVarNames() %T>% print()
#Fix specific variable names
names(ds)[23] <- c("rainfall_tomorrow")
#Check the names
names(ds)
#Note any identifiers
id <- c("date", "location")
#Note the target variable
target <- "rain_tomorrow"
#Note any risk variable -measures the severity of the outcome
risk <- "rainfall_tomorrow"
#Note available variable ignoring identifiers and risk, with target first
ds %>%
names() %>%
setdiff(c(id,risk)) %>%
c(target, .) %>%
unique() %T>%
print()->
vars
#We will want to ignore some variables that are not relevant to our analysis
ignore <- NULL
#Remove variable to ignore from the variable list
vars%<>%setdiff(ignore) %T>% print()
#Deal with numeric variables
#Fix specific numeric variables
cvars <- c("evaporation", "sunshine")
ds[cvars] %<>% sapply(as.numeric)
#Remove observations without a value for rain_tomorrow
ds%<>% drop_na(rain_tomorrow)
#Identify character variables by index
chari <-ds[vars] %>% sapply(is.character) %>% which() %T>% print()
#Identify character variables by name
charc <-ds[vars] %>% names() %>% '['(chari) %T>% print()
#Observe unique levels
ds[charc] %>% sapply (unique)
#Convert all character to factor if determined appropriate
ds[charc]%<>%map(factor)
#Take a quick glimpse into the dataset
glimpse(ds)
#Visualise relationship in the dataset
ds %>%
filter(location%in%c("Canberra", "Darwin", "Sydney")) %>%
filter(temp_3pm %>% is.na() %>% not()) %>%
select(temp_3pm, location) %>%
ggplot(aes(x=temp_3pm, colour=location, fill=location))+
geom_density(alpha=0.55)
#Formula for modelling
ds[vars] %>%
formula() %T>%
print() ->
form
#Ensure that the target is categoric
ds[[target]] %<>% factor()
#Identify the input variables by name
inputs <-setdiff(vars,target) %T>% print()
#Record the number of observations
nobs <- nrow(ds) %T>% comcat()
#Initialise random numbers for repeatedable results
seed <-123
set.seed(seed)
#Partition the full dataset into three: train(70%), validate (15%), test(15%)
nobs %>%
sample(0.70*nobs) %T>%
{length(.) %>% comma() %>% cat("Size of training dataset:",.,"\n")} ->
train
#Create a validation dataset of 15% of the observations
nobs %>%
seq_len() %>%
setdiff(train) %>%
sample(0.15*nobs) %T>%
{length(.) %>% comma() %>% cat("Size of validation dataset:",.,"\n")}->
validate
#Create a testing dataset of 15%(the remainder) of the observations
nobs %>%
seq_len() %>%
setdiff(union(train, validate)) %T>%
{length(.) %>% comma() %>% cat("Size of validation dataset:",.,"\n")}->
test
#Cache the various actual values for target and risk
tr_target <-ds[train,][[target]] %T>% {head(., 15) %>% print()}
tr_risk <-ds[train,][[risk]] %T>% {head(., 15) %>% print()}
va_target <-ds[validate,][[target]] %T>% {head(., 15) %>% print()}
va_risk <- ds[validate,][[risk]] %T>% {head(., 15) %>% print()}
te_target <-ds[test,][[target]] %T>% {head(., 15) %>% print()}
te_risk <- ds[test,][[risk]] %T>% {head(., 15) %>% print()}
#splitting function: "anova" "poisson" "class" "exp"
mthd <- "class"
#Splitting function parameters
prms <- list(split= "information")
#Control the training
ctrl <-rpart.control(maxdepth=5)
#Build the model
m_rp <-rpart(form, ds[train, vars], method=mthd, parms=prms, control=ctrl)
library(modelr)
#Control the training
ctrl <-rpart.control(maxdepth=5)
?rpart.control
??rpart.control
??modelr
??FSelector
library(magrittr)
library(lubridate)
library(rattle)
library(ROCR)
library(rpart)
library(scales)
library(stringi)
library(scales)
library(tidyverse)
#I am building a decision tree model to predict future events from historic observations of that event.
# The event is whether it will rain tomorrow.
#Load required packages from local library into R
library(magrittr) #Pipe operator %>% %<>% %T>% equal().
library(lubridate) # Dates and time.
library(rattle) #normVarnames().
library(ROCR) # Use prediction for evaluation.
library(rpart)  #Model: decision tree.
library(scales) #Include commas in numbers.
library(stringi) #String concat %s+% operator.
library(tidyverse) #ggplot2, tibble, tidyr,readr,purr, dplyr, stringr
#Original dataset source/location
dsorig <- file.path("https://rattle.togaware.com/weatherAUS.csv")
#Name of the dataset
dsname <-"weatherAUS"
#Identify the essential location of the dataset
dsloc <- "C:/Users/EDJO/Documents/Data Cleaning/Data"
dspath <- file.path(dsloc, dsname%s+% ".csv") %T>% print()
#Ingest the dataset
dspath %>% read_csv() %>% assign(dsname, ., envir=.GlobalEnv)
#Store the dataset with a generic template variable name
dsname %>% get() %T>% print() -> ds
#Normalise variable names
names(ds) %<>% normVarNames() %T>% print()
#Fix specific variable names
names(ds)[23] <- c("rainfall_tomorrow")
#Check the names
names(ds)
#Note any identifiers
id <- c("date", "location")
#Note the target variable
target <- "rain_tomorrow"
#Note any risk variable -measures the severity of the outcome
risk <- "rainfall_tomorrow"
#Note available variable ignoring identifiers and risk, with target first
ds %>%
names() %>%
setdiff(c(id,risk)) %>%
c(target, .) %>%
unique() %T>%
print()->
vars
#We will want to ignore some variables that are not relevant to our analysis
ignore <- NULL
#Remove variable to ignore from the variable list
vars%<>%setdiff(ignore) %T>% print()
#Deal with numeric variables
#Fix specific numeric variables
cvars <- c("evaporation", "sunshine")
ds[cvars] %<>% sapply(as.numeric)
#Remove observations without a value for rain_tomorrow
ds%<>% drop_na(rain_tomorrow)
#Identify character variables by index
chari <-ds[vars] %>% sapply(is.character) %>% which() %T>% print()
#Identify character variables by name
charc <-ds[vars] %>% names() %>% '['(chari) %T>% print()
#Observe unique levels
ds[charc] %>% sapply (unique)
#Convert all character to factor if determined appropriate
ds[charc]%<>%map(factor)
#Take a quick glimpse into the dataset
glimpse(ds)
#Visualise relationship in the dataset
ds %>%
filter(location%in%c("Canberra", "Darwin", "Sydney")) %>%
filter(temp_3pm %>% is.na() %>% not()) %>%
select(temp_3pm, location) %>%
ggplot(aes(x=temp_3pm, colour=location, fill=location))+
geom_density(alpha=0.55)
#Formula for modelling
ds[vars] %>%
formula() %T>%
print() ->
form
#Ensure that the target is categoric
ds[[target]] %<>% factor()
#Identify the input variables by name
inputs <-setdiff(vars,target) %T>% print()
#Record the number of observations
nobs <- nrow(ds) %T>% comcat()
#Initialise random numbers for repeatedable results
seed <-123
set.seed(seed)
#Partition the full dataset into three: train(70%), validate (15%), test(15%)
nobs %>%
sample(0.70*nobs) %T>%
{length(.) %>% comma() %>% cat("Size of training dataset:",.,"\n")} ->
train
#Create a validation dataset of 15% of the observations
nobs %>%
seq_len() %>%
setdiff(train) %>%
sample(0.15*nobs) %T>%
{length(.) %>% comma() %>% cat("Size of validation dataset:",.,"\n")}->
validate
#Create a testing dataset of 15%(the remainder) of the observations
nobs %>%
seq_len() %>%
setdiff(union(train, validate)) %T>%
{length(.) %>% comma() %>% cat("Size of validation dataset:",.,"\n")}->
test
#Cache the various actual values for target and risk
tr_target <-ds[train,][[target]] %T>% {head(., 15) %>% print()}
tr_risk <-ds[train,][[risk]] %T>% {head(., 15) %>% print()}
va_target <-ds[validate,][[target]] %T>% {head(., 15) %>% print()}
va_risk <- ds[validate,][[risk]] %T>% {head(., 15) %>% print()}
te_target <-ds[test,][[target]] %T>% {head(., 15) %>% print()}
te_risk <- ds[test,][[risk]] %T>% {head(., 15) %>% print()}
#splitting function: "anova" "poisson" "class" "exp"
mthd <- "class"
#Splitting function parameters
prms <- list(split= "information")
#Control the training
ctrl <-rpart.control(maxdepth=5)
#Build the model
m_rp <-rpart(form, ds[train, vars], method=mthd, parms=prms, control=ctrl)
??FSelector
#Review Model----
#Basic model structure.
model
#Review Model----
#Basic model structure.
model
#Model Generic Variables----
#Capture the model in generic variable.
model <- m_rp
mtype <- "rpart"
mdesc <- "Decision Tree"
#Review Model----
#Basic model structure.
model
#Visualize the model----
#Visually expose the discovered knowledge.
fancyRpartPlot(model)
ds %>%
filter(location%in%c("Canberra", "Darwin", "Sydney")) %>%
filter(temp_3pm %>% is.na() %>% not()) %>%
select(temp_3pm, location) %>%
ggplot(aes(x=temp_3pm, colour=location, fill=location))+
geom_density(alpha=0.55)
#Summary of the Model----
#Complete the model build summary.
summary(model)
#Variable Importance
#Review the rleative importance of the variables.
ggVarImp(model)
model %>%
predict(newdata=ds[validate, vars], type="class") %>%
set_names(NULL) %T>%
{head(., 20) %>% print()} ->
va_class
model %>%
predict(newdata=ds[validate, vars], type="prob") %>%
.[,2] %>%
set_names(NULL) %>%
round(2) %T>%
{head(., 20) %>% print() } ->
va_prob
#Overall Accuracy and Error----
#Assess model overall accuracy and error.
sum(va_class == va_target) %>%
divide_by(length(va_target)) %T>%
{
percent(.) %>%
sprintf("Overall accuracy=%s\n",.) %>%
cat()
} ->
va_acc
#Assess model error.
sum(va_class!=va_target) %>%
divide_by(length(va_target)) %T>%
{
percent(.) %>%
sprintf("Overall error=%s\n",.) %>%
cat()
} ->
va_err
#Confusion Matrix----
#Basic comparison of prediction/actual as a confusion matrix.
table(va_target, va_class, useNA="ifany", dnn=c("Actual", "Predicted"))
#Comparison as percentages of all observations.
errorMatrix(va_target, va_class) %T>%
print() ->
va_matrix
#Error rate and average of the class error rate.
va_matrix %>%
diag() %>%
sum(na.rm=TRUE) %>%
subtract(100, .) %>%
sprintf("Overall error percentage=%s%%\n", .) %>%
cat()
va_matrix[, "Error"] %>%
mean(na.rm=TRUE) %>%
sprintf("Averaged class error percentage=%s%%\n",.) %>%
cat()
#Recall, Preicision and F-Score----
#Assess other performance metrics.
va_rec <- (va_matrix[2,2]/(va_matrix[2,2]+va_matrix[2,1])) %T>%
{percent(.) %>% sprintf("Recall=%s\n",.) %>% cat()}
va_pre <- (va_matrix[2,2]/(va_matrix[2,2]+va_matrix[1,2])) %T>%
{percent(.) %>% sprintf("Precision=%s\n",.) %>% cat()}
va_fsc <-((2*va_pre*va_rec)/(va_rec+ va_pre)) %T>%
{sprintf("F-Score=%.3f\n",.) %>% cat()}
#Caluclate the area under the curve.
va_prob %>%
prediction(va_target) %>%
performance("auc") %>%
attr("y.values") %>%
.[[1]] %T>%
{
percent(.) %>%
sprintf("Percentage area under the ROC curve=%s\n",.) %>%
cat()
}->
va_auc
#Calculate measures needed to plot ROC Curve
va_prob %>%
prediction(va_target) %>%
performance("tpr", "fpr")->
va_rates
#ROC Curve Plot----
#Plot the ROC.
data_frame(tpr=attr(va_rates, "y.values")) [[1]],
fpr= attr(va_rates,"x.value")[[1]]) %>%
ggplot(aes(fpr, tpr)) +
geom_line()+
annotate("text", x=0.875, y=0.125, vjust=0,
label=paste("AUC=", percent(va_auc))) +
labs(title="ROC-" %s+% mtype %s+% "validation Dataset",
x="False Positive Rate(1-Specificity"),
y="True Positive Rate (Sensitivity)")
#ROC Curve Plot----
#Plot the ROC.
data_frame(tpr=attr(va_rates, "y.values")) [[1]],
fpr= attr(va_rates,"x.value")[[1]]) %>%
ggplot(aes(fpr, tpr)) +
geom_line()+
annotate("text", x=0.875, y=0.125, vjust=0,
label=paste("AUC=", percent(va_auc))) +
labs(title="ROC-" %s+% mtype%s+% "-Validation Dataset",
x="False Positive Rate(1-Specificity"),
y="True Positive Rate (Sensitivity)")
#ROC Curve Plot----
#Plot the ROC.
data_frame(tpr=attr(va_rates, "y.values")) [[1]],
fpr= attr(va_rates,"x.value")[[1]]) %>%
ggplot(aes(fpr, tpr)) +
geom_line()+
annotate("text", x=0.875, y=0.125, vjust=0,
label=paste("AUC=", percent(va_auc))) +
labs(title="ROC - " %s+% mtype %s+% " - Validation Dataset",
x="False Positive Rate (1-Specificity)",
y="True Positive Rate (Sensitivity)")
#ROC Curve Plot----
#Plot the ROC.
data_frame(tpr=attr(va_rates, "y.values")) [[1]],
fpr= attr(va_rates,"x.value")[[1]]) %>%
ggplot(aes(fpr, tpr)) +
geom_line()+
annotate("text", x=0.875, y=0.125, vjust=0,
label=paste("AUC=", percent(va_auc))) +
labs(title="ROC - " %s+% mtype %s+% " - Validation Dataset",
x="False Positive Rate (1-Specificity)",
y="True Positive Rate (Sensitivity)")
#ROC Curve Plot----
#Plot the ROC.
data_frame(tpr=attr(va_rates, "y.values")) [[1]],
fpr= attr(va_rates,"x.value")[[1]]) %>%
ggplot(aes(fpr, tpr)) +
geom_line()+
annotate("text", x=0.875, y=0.125, vjust=0,
label=paste("AUC=", percent(va_auc))) +
labs(title="ROC - " %s+% mtype %s+% " - Validation Dataset",
x="False Positive Rate (1-Specificity)",
y="True Positive Rate (Sensitivity)")
#Plot the ROC.
data_frame(tpr=attr(va_rates, "y.values")[[1]],
fpr=attr(va_rates, "x.values")[[1]]) %>%
ggplot(aes(fpr, tpr)) +
geom_line() +
annotate("text", x=0.875, y=0.125, vjust=0,
label=paste("AUC =", percent(va_auc))) +
labs(title="ROC - " %s+% mtype %s+% " - Validation Dataset",
x="False Positive Rate (1-Specificity)",
y="True Positive Rate (Sensitivity)")
#Risk Chart----
#Plot risk chart.
riskchart(va_prob, va_target, va_risk) +
labs(title="Risk Chart-" %s+%
mtype %s+%
"-Validation Dataset") +
theme(plot.title=element_text(size=14))
